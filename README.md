# demo-crawler4j

Some data may be not provided by APIs, but distributed among html pages; is it possible to extract and formalise them programmatically?

## What is Web Crawler

It is an internet bot or automatic indexer or web robot. 

* Update their web content or indices from others sites' web content
* Copy all the visited pages for subsequent processing by a search engine which will index the downloaded pages to provide lightening fast searches
* Automate maintenance tasks on a web site, such as checking links or validating HTML code

### How Crawler Works
Step1: start from a list of URLs
Step2: Fetch and parse URLs
Step3: Extract URLs they point to
Step4: Place the extracted URLs on Q
Step5: Fetch each URL and repeating the same

